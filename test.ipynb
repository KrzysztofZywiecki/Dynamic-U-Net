{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from networks import DynamicUNet, LearnableScaleDynamicUNet\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import re\n",
    "from utils import *\n",
    "\n",
    "ids = [ re.findall(\"[0-9]+\", name)[0] for name in os.listdir(\"./segmentation/patches\") ]\n",
    "\n",
    "rest, test = train_test_split(ids, test_size=20, random_state=42)\n",
    "test_dataset = SegmentationDataset(test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False, num_workers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "batch_size = 10\n",
    "train_loss_fn = nn.CrossEntropyLoss()\n",
    "valid_loss_fn = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "plateau_window = 5\n",
    "scales = [1/8, 1/4, 1/2, 1]\n",
    "n_runs = 10\n",
    "\n",
    "\n",
    "losses = [ [ [] for _ in scales ] for _ in range(n_runs) ]\n",
    "dice_scores = [ [ [] for _ in scales ] for _ in range(n_runs) ]\n",
    "test_scores = []\n",
    "\n",
    "\n",
    "for n in range(n_runs):\n",
    "    network = LearnableScaleDynamicUNet(1, 2, [128, 256, 512], 8, [(64, 4), (32, 2), (16, 1)]).to(device)\n",
    "    # network = DynamicUNet(1, 2, [128, 256, 512], 8, [(64, 4), (32, 2), (16, 1)]).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(network.parameters(), lr=learning_rate)\n",
    "\n",
    "    for i, scale in enumerate(scales):\n",
    "        train, validation = train_test_split(rest, test_size=20, random_state=42)\n",
    "\n",
    "        train_dataset = SegmentationDataset(train, 1, 1)\n",
    "        validation_dataset = SegmentationDataset(validation, 1, 1)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=12)\n",
    "        validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, num_workers=12)\n",
    "        breakthrough = 999\n",
    "        since = 0\n",
    "        side = 128\n",
    "\n",
    "        while since != plateau_window:\n",
    "            # Training\n",
    "            for X, y in train_loader:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                x_hat = network(X)\n",
    "                loss = train_loss_fn(x_hat, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            # Validating\n",
    "            with torch.no_grad():\n",
    "                loss = 0\n",
    "                dice_score_ = 0\n",
    "                for X, y in validation_loader:\n",
    "                    X, y = X.to(device,), y.to(device)\n",
    "                    x_hat = network(X)\n",
    "                    loss += valid_loss_fn(x_hat, y).item()\n",
    "                    dice_score_ += dice_score(x_hat.cpu(), y.cpu(), reduction=\"sum\")\n",
    "                loss = loss / (len(validation_dataset) * side * side ) \n",
    "                dice_score_ = dice_score_ / len(validation_dataset)\n",
    "                dice_scores[n][i].append(dice_score_)\n",
    "                since += 1\n",
    "                if loss < breakthrough:\n",
    "                    breakthrough = loss\n",
    "                    since=0\n",
    "                print(\"Loss:\", loss, \"Dice score:\", dice_score_, \"N:\", n)\n",
    "                losses[n][i].append(loss)\n",
    "        print(\"Epoch ended\")\n",
    "        network.use_higher_layer()\n",
    "    # Testing\n",
    "    with torch.no_grad():\n",
    "        dice_score_ = 0\n",
    "        for X, y in test_loader:\n",
    "            X, y = X.to(device,), y.to(device)\n",
    "            x_hat = network(X)\n",
    "            dice_score_ += dice_score(x_hat.cpu(), y.cpu(), reduction=\"sum\")\n",
    "        dice_score_ = dice_score_ / len(test_dataset)\n",
    "        test_scores.append(dice_score_)\n",
    "        print(dice_score_)\n",
    "    print(\"Run ended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "colors = [\"r\", \"g\", \"b\", \"orange\"]\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "for run in losses:\n",
    "    for loss, color in zip(run, colors):\n",
    "        plt.plot(loss, color)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "for run in dice_scores:\n",
    "    for dice, color in zip(run, colors):\n",
    "        plt.plot(dice, color)\n",
    "plt.legend([1,2,3,4])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postfix = \"ls_fc\"\n",
    "\n",
    "with open(\"loss_val_\"+postfix, \"w\") as file:\n",
    "    for i, run in enumerate(losses):\n",
    "        file.write(\"run {}\\n\".format(i))\n",
    "        for j, model in enumerate(run):\n",
    "            file.write(\"model {}\\n\".format(j))\n",
    "            file.writelines([str(element)+\"\\n\" for element in model])\n",
    "\n",
    "with open(\"dice_val_\"+postfix, \"w\") as file:\n",
    "    for i, run in enumerate(dice_scores):\n",
    "        file.write(\"run {}\\n\".format(i))\n",
    "        for j, model in enumerate(run):\n",
    "            file.write(\"model {}\\n\".format(j))\n",
    "            file.writelines([str(element)+\"\\n\" for element in model])\n",
    "\n",
    "with open(\"dice_test_\"+postfix, \"w\") as file:\n",
    "    file.writelines([str(element)+\"\\n\" for element in test_scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def read_file(filename):\n",
    "    with open(filename, \"r\") as file:\n",
    "        for line in file.readlines()[2:]:\n",
    "            if re.match(\"run\", line) != None:\n",
    "                yield \"nr\"\n",
    "            elif re.match(\"model\", line) != None:\n",
    "                yield \"nm\"\n",
    "            else:\n",
    "                yield float(line)   \n",
    "\n",
    "def parse(filename):\n",
    "    file = read_file(filename)\n",
    "    structure = []\n",
    "    run_st = []\n",
    "    model_st = []\n",
    "    for read in file:\n",
    "        if read == \"nm\":\n",
    "            run_st.append(model_st)\n",
    "            model_st = []\n",
    "        elif read == \"nr\":\n",
    "            structure.append(run_st)\n",
    "            model_st = []\n",
    "            run_st = []\n",
    "        else:\n",
    "            model_st.append(read)\n",
    "    return structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "structure = parse(\"dice_val_s\")\n",
    "\n",
    "final_scores = [run[-1][-1] for run in structure]\n",
    "np.mean(final_scores)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
